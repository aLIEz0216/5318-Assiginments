{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group number: SID1: 530335425 , SID2: 530039187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "#df = pd.read_csv(\"test-before.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filling in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "\n",
    "# remove 'class' column\n",
    "df_without_class = df.loc[:, df.columns != df.columns[-1]]\n",
    "\n",
    "# change dataframe into numeric type\n",
    "df_without_class = df_without_class.apply(lambda x: pd.to_numeric(x, errors='coerce'), axis= 1)\n",
    "\n",
    "# create SimpleImputer, fit and transfrom data, change it back to df\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "array_imputed = imp_mean.fit_transform(df_without_class)\n",
    "df_imputed = pd.DataFrame(array_imputed, columns=df_without_class.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Normalising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MinMaxScaler, fit and transfrom data\n",
    "scaler = MinMaxScaler()\n",
    "array_normalized = scaler.fit_transform(df_imputed)\n",
    "df_normalized = pd.DataFrame(array_normalized, columns=df_without_class.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Changing the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class1 to 0, class2 to 1\n",
    "df_class = df[df.columns[-1]].apply(lambda x: 0 if x == 'class1' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Print the first 10 rows of the pre-processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444,0.0000,0.0000,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.4444,0.3333,0.3333,0.4444,0.6667,1.0000,0.2222,0.1111,0.0000,0\n",
      "0.2222,0.0000,0.0000,0.0000,0.1111,0.1111,0.2222,0.0000,0.0000,0\n",
      "0.5556,0.7778,0.7778,0.0000,0.2222,0.3333,0.2222,0.6667,0.0000,0\n",
      "0.3333,0.0000,0.0000,0.2222,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.7778,1.0000,1.0000,0.7778,0.6667,1.0000,0.8889,0.6667,0.0000,1\n",
      "0.0000,0.0000,0.0000,0.0000,0.1111,1.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.1111,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.0000,0.0000,0.1111,0.0000,0.0000,0.0000,0.4444,0\n",
      "0.3333,0.1111,0.0000,0.0000,0.1111,0.0000,0.1111,0.0000,0.0000,0\n"
     ]
    }
   ],
   "source": [
    "print_data(df_normalized.to_numpy(), df_class.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# The stratified folds from cvKFold should be provided to the classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logregClassifier(X, y, cvKFold):\n",
    "    clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "    scores = cross_val_score(clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y, cvKFold):\n",
    "    nb = GaussianNB()\n",
    "    scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y, cvKFold):\n",
    "    dt = DecisionTreeClassifier(criterion='entropy',random_state=0)\n",
    "    scores = cross_val_score(dt, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth, cvKFold):\n",
    "    bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(criterion='entropy', random_state=0, max_depth=max_depth), n_estimators=n_estimators, \n",
    "        max_samples=max_samples, bootstrap=True, random_state=0)\n",
    "    scores = cross_val_score(bag_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth, cvKFold):\n",
    "    ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(criterion='entropy', max_depth=max_depth), n_estimators=n_estimators, \n",
    "        learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(ada_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate, cvKFold):\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(gb_clf, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9642\n",
      "NB average cross-validation accuracy: 0.9585\n",
      "DT average cross-validation accuracy: 0.9385\n",
      "Bagging average cross-validation accuracy: 0.9571\n",
      "AdaBoost average cross-validation accuracy: 0.9570\n",
      "GB average cross-validation accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# assign the data to X, y\n",
    "X = df_normalized.to_numpy()\n",
    "y = df_class.to_numpy()\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "LogR_score = logregClassifier(X, y, cvKFold)\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(LogR_score))\n",
    "NB_score = nbClassifier(X, y, cvKFold)\n",
    "print(\"NB average cross-validation accuracy: {:.4f}\".format(NB_score))\n",
    "dt_score = dtClassifier(X, y, cvKFold)\n",
    "print(\"DT average cross-validation accuracy: {:.4f}\".format(dt_score))\n",
    "bagDT_score = bagDTClassifier(X, y, bag_n_estimators, bag_max_samples, bag_max_depth, cvKFold)\n",
    "print(\"Bagging average cross-validation accuracy: {:.4f}\".format(bagDT_score))\n",
    "AdaBoost_score = adaDTClassifier(X, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth, cvKFold)\n",
    "print(\"AdaBoost average cross-validation accuracy: {:.4f}\".format(AdaBoost_score))\n",
    "GB_score = gbClassifier(X, y, gb_n_estimators, gb_learning_rate, cvKFold)\n",
    "print(\"GB average cross-validation accuracy: {:.4f}\".format(GB_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y, cvKFold, k, p):\n",
    "    # create param grid\n",
    "    param_grid = {'n_neighbors' : k, 'p': p}\n",
    "\n",
    "    # train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # create grid and fit data (return grid_search)\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # test set accuracy(return test_score)\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    return grid_search, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5, 15] \n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "\n",
    "def bestSVMClassifier(X, y, cvKFold, C, gamma):\n",
    "    # create param grid\n",
    "    param_grid = {'C' : C, 'gamma': gamma}\n",
    "\n",
    "    # train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # create grid and fit data (return grid_search)\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # test set accuracy(return test_score)\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "    return  grid_search, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "\n",
    "def bestRFClassifier(X, y, cvKFold, n_estimators, max_leaf_nodes):\n",
    "    # create param grid\n",
    "    param_grid = {'n_estimators' : n_estimators, 'max_leaf_nodes': max_leaf_nodes}\n",
    "    \n",
    "    # train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    # create grid and fit data (return grid_search)\n",
    "    rnf_clf = RandomForestClassifier(criterion='entropy', max_features = 'sqrt',random_state=0)\n",
    "    grid_search = GridSearchCV(rnf_clf, param_grid, cv=cvKFold, return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # test set accuracy(return test_score)\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "    # macro_f1, weighted_f1 for rnf with best params (return macro_f1, weighted_f1)\n",
    "    rnf_clf_best = RandomForestClassifier(criterion='entropy', max_features = 'sqrt',random_state=0, \n",
    "                                          max_leaf_nodes = grid_search.best_params_['max_leaf_nodes'],\n",
    "                                          n_estimators = grid_search.best_params_['n_estimators'])\n",
    "    # fit and predict data with best rnf_clf\n",
    "    rnf_clf_best.fit(X_train, y_train)\n",
    "    y_pred = rnf_clf_best.predict(X_test)\n",
    "\n",
    "    # get the macro_f1 and weighted_f1\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    return grid_search, test_score, macro_f1, weighted_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 3\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9695\n",
      "KNN test set accuracy: 0.9543\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 0.1000\n",
      "SVM cross-validation accuracy: 0.9676\n",
      "SVM test set accuracy: 0.9714\n",
      "\n",
      "RF best n_estimators: 150\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.9675\n",
      "RF test set accuracy: 0.9657\n",
      "RF test set macro average F1: 0.9628\n",
      "RF test set weighted average F1: 0.9661\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "\n",
    "# assign the data to X, y\n",
    "X = df_normalized.to_numpy()\n",
    "y = df_class.to_numpy()\n",
    "\n",
    "# KNN model\n",
    "KNN_grid_search,KNN_test_score = bestKNNClassifier(X, y, cvKFold, k, p)\n",
    "print(\"KNN best k: {}\".format(KNN_grid_search.best_params_['n_neighbors']))\n",
    "print(\"KNN best p: {}\".format(KNN_grid_search.best_params_['p']))\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(KNN_grid_search.best_score_))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(KNN_test_score))\n",
    "\n",
    "print()\n",
    "# SVM model\n",
    "SVM_grid_search, SVM_test_score = bestSVMClassifier(X, y, cvKFold, C, gamma)\n",
    "print(\"SVM best C: {:.4f}\".format(SVM_grid_search.best_params_['C']))\n",
    "print(\"SVM best gamma: {:.4f}\".format(SVM_grid_search.best_params_['gamma']))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(SVM_grid_search.best_score_))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(SVM_test_score))\n",
    "\n",
    "print()\n",
    "# rnf model\n",
    "rnf_grid_search, rnf_test_score, macro_f1, weighted_f1 = bestRFClassifier(X, y, cvKFold, n_estimators, max_leaf_nodes)\n",
    "print(\"RF best n_estimators: {}\".format(rnf_grid_search.best_params_['n_estimators']))\n",
    "print(\"RF best max_leaf_nodes: {}\".format(rnf_grid_search.best_params_['max_leaf_nodes']))\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rnf_grid_search.best_score_))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rnf_test_score))\n",
    "print(\"RF test set macro average F1: {:.4f}\".format(macro_f1))\n",
    "print(\"RF test set weighted average F1: {:.4f}\".format(weighted_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
